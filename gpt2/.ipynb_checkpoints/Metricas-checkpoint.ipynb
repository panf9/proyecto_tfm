{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e6e77-d4a8-48e7-beab-4742832b0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8724f-3d16-43e7-875f-a6352ae6d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from evaluate import load as load_metric\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f17b40-03d4-4443-910b-590d12367e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci贸n\n",
    "MODEL_PATH = \"./modelo\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    ").to(DEVICE)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bb748-7648-472b-a546-4273a20a6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset completo\n",
    "data_files = {\n",
    "    'train': 'dataset_train_filtrado.jsonl',\n",
    "    'validation': 'dataset_valid_filtrado.jsonl',\n",
    "    'test': 'dataset_test_filtrado.jsonl'\n",
    "}\n",
    "raw_datasets = load_dataset('json', data_files=data_files)\n",
    "test_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "# Tomar solo la tercera parte\n",
    "subset_size = len(test_dataset)\n",
    "test_subset = test_dataset.select(range(subset_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc57c2-5526-429c-aad8-000473536f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comentar_codigo(snippet):\n",
    "    prompt = f\"Comenta este c贸digo Python:\\n```python\\n{snippet}\\n```\\nC贸digo comentado:\\n```python\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    comentado = full_output.split(\"C贸digo comentado:\")[-1].strip()\n",
    "    return comentado.replace(\"```\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4d866-cc97-4e4a-850d-43fe5ec7696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "meteor = load_metric(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e8db9-2d71-4550-8633-02772699b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs, preds = [], []\n",
    "\n",
    "for item in tqdm(test_subset):\n",
    "    codigo_limpio = item[\"code_clean\"]\n",
    "    codigo_comentado_ref = item[\"code\"]\n",
    "\n",
    "    codigo_comentado_pred = comentar_codigo(codigo_limpio)\n",
    "\n",
    "    refs.append(codigo_comentado_ref)\n",
    "    preds.append(codigo_comentado_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4a429-13f2-45e5-af56-b23f89ad656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_scores = [\n",
    "    sentence_bleu(\n",
    "        [ref.split()],\n",
    "        pred.split(),\n",
    "        smoothing_function=smoothie\n",
    "    ) for ref, pred in zip(refs, preds)\n",
    "]\n",
    "bleu_avg = np.mean(bleu_scores)\n",
    "\n",
    "# ROUGE\n",
    "rouge_result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "\n",
    "# METEOR\n",
    "meteor_result = meteor.compute(predictions=preds, references=refs)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\nBLEU score promedio: {bleu_avg:.4f}\")\n",
    "print(f\"METEOR: {meteor_result['meteor']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
