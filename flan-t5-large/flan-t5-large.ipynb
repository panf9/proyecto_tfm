{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685cf5c9-36cb-461a-8390-c4ad81745c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9360a13-9c10-4fcf-90ea-d0f235d9cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Paso 2: Importaciones\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09c139c-e35e-4ec1-b6aa-e109589415de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'repo': ['titilambert/pyfido', 'titilambert/pyfido'], 'path': ['pyfido/client.py', 'pyfido/client.py'], 'func_name': ['FidoClient._post_login_page', 'FidoClient._get_account_number'], 'original_string': ['def _post_login_page(self):\\n        \"\"\"Login to Janrain.\"\"\"\\n        # Prepare post data\\n        data = {\\n            \"form\": \"signInForm\",\\n            \"client_id\": JANRAIN_CLIENT_ID,\\n            \"redirect_uri\": \"https://www.fido.ca/pages/#/\",\\n            \"response_type\": \"token\",\\n            \"locale\": \"en-US\",\\n            \"userID\": self.username,\\n            \"currentPassword\": self.password,\\n        }\\n        # HTTP request\\n        try:\\n            raw_res = yield from self._session.post(LOGIN_URL,\\n                                                    headers=self._headers,\\n                                                    data=data,\\n                                                    timeout=self._timeout)\\n        except OSError:\\n            raise PyFidoError(\"Can not sign in\")\\n\\n        return True', 'def _get_account_number(self, token, uuid):\\n        \"\"\"Get fido account number.\"\"\"\\n        # Data\\n        data = {\"accessToken\": token,\\n                \"uuid\": uuid}\\n        # Http request\\n        try:\\n            raw_res = yield from self._session.post(ACCOUNT_URL,\\n                                                    data=data,\\n                                                    headers=self._headers,\\n                                                    timeout=self._timeout)\\n        except OSError:\\n            raise PyFidoError(\"Can not get account number\")\\n        # Load answer as json\\n        try:\\n            json_content = yield from raw_res.json()\\n            account_number = json_content\\\\\\n                            .get(\\'getCustomerAccounts\\', {})\\\\\\n                            .get(\\'accounts\\', [{}])[0]\\\\\\n                            .get(\\'accountNumber\\')\\n        except (OSError, ValueError):\\n            raise PyFidoError(\"Bad json getting account number\")\\n        # Check collected data\\n        if account_number is None:\\n            raise PyFidoError(\"Can not get account number\")\\n\\n        return account_number'], 'language': ['python', 'python'], 'code': ['def _post_login_page(self):\\n        \"\"\"Login to Janrain.\"\"\"\\n        # Prepare post data\\n        data = {\\n            \"form\": \"signInForm\",\\n            \"client_id\": JANRAIN_CLIENT_ID,\\n            \"redirect_uri\": \"https://www.fido.ca/pages/#/\",\\n            \"response_type\": \"token\",\\n            \"locale\": \"en-US\",\\n            \"userID\": self.username,\\n            \"currentPassword\": self.password,\\n        }\\n        # HTTP request\\n        try:\\n            raw_res = yield from self._session.post(LOGIN_URL,\\n                                                    headers=self._headers,\\n                                                    data=data,\\n                                                    timeout=self._timeout)\\n        except OSError:\\n            raise PyFidoError(\"Can not sign in\")\\n\\n        return True', 'def _get_account_number(self, token, uuid):\\n        \"\"\"Get fido account number.\"\"\"\\n        # Data\\n        data = {\"accessToken\": token,\\n                \"uuid\": uuid}\\n        # Http request\\n        try:\\n            raw_res = yield from self._session.post(ACCOUNT_URL,\\n                                                    data=data,\\n                                                    headers=self._headers,\\n                                                    timeout=self._timeout)\\n        except OSError:\\n            raise PyFidoError(\"Can not get account number\")\\n        # Load answer as json\\n        try:\\n            json_content = yield from raw_res.json()\\n            account_number = json_content\\\\\\n                            .get(\\'getCustomerAccounts\\', {})\\\\\\n                            .get(\\'accounts\\', [{}])[0]\\\\\\n                            .get(\\'accountNumber\\')\\n        except (OSError, ValueError):\\n            raise PyFidoError(\"Bad json getting account number\")\\n        # Check collected data\\n        if account_number is None:\\n            raise PyFidoError(\"Can not get account number\")\\n\\n        return account_number'], 'code_tokens': [['def', '_post_login_page', '(', 'self', ')', ':', '# Prepare post data', 'data', '=', '{', '\"form\"', ':', '\"signInForm\"', ',', '\"client_id\"', ':', 'JANRAIN_CLIENT_ID', ',', '\"redirect_uri\"', ':', '\"https://www.fido.ca/pages/#/\"', ',', '\"response_type\"', ':', '\"token\"', ',', '\"locale\"', ':', '\"en-US\"', ',', '\"userID\"', ':', 'self', '.', 'username', ',', '\"currentPassword\"', ':', 'self', '.', 'password', ',', '}', '# HTTP request', 'try', ':', 'raw_res', '=', 'yield', 'from', 'self', '.', '_session', '.', 'post', '(', 'LOGIN_URL', ',', 'headers', '=', 'self', '.', '_headers', ',', 'data', '=', 'data', ',', 'timeout', '=', 'self', '.', '_timeout', ')', 'except', 'OSError', ':', 'raise', 'PyFidoError', '(', '\"Can not sign in\"', ')', 'return', 'True'], ['def', '_get_account_number', '(', 'self', ',', 'token', ',', 'uuid', ')', ':', '# Data', 'data', '=', '{', '\"accessToken\"', ':', 'token', ',', '\"uuid\"', ':', 'uuid', '}', '# Http request', 'try', ':', 'raw_res', '=', 'yield', 'from', 'self', '.', '_session', '.', 'post', '(', 'ACCOUNT_URL', ',', 'data', '=', 'data', ',', 'headers', '=', 'self', '.', '_headers', ',', 'timeout', '=', 'self', '.', '_timeout', ')', 'except', 'OSError', ':', 'raise', 'PyFidoError', '(', '\"Can not get account number\"', ')', '# Load answer as json', 'try', ':', 'json_content', '=', 'yield', 'from', 'raw_res', '.', 'json', '(', ')', 'account_number', '=', 'json_content', '.', 'get', '(', \"'getCustomerAccounts'\", ',', '{', '}', ')', '.', 'get', '(', \"'accounts'\", ',', '[', '{', '}', ']', ')', '[', '0', ']', '.', 'get', '(', \"'accountNumber'\", ')', 'except', '(', 'OSError', ',', 'ValueError', ')', ':', 'raise', 'PyFidoError', '(', '\"Bad json getting account number\"', ')', '# Check collected data', 'if', 'account_number', 'is', 'None', ':', 'raise', 'PyFidoError', '(', '\"Can not get account number\"', ')', 'return', 'account_number']], 'docstring': ['Login to Janrain.', 'Get fido account number.'], 'docstring_tokens': [['Login', 'to', 'Janrain', '.'], ['Get', 'fido', 'account', 'number', '.']], 'sha': ['8302b76f4d9d7b05b97926c003ca02409aa23281', '8302b76f4d9d7b05b97926c003ca02409aa23281'], 'url': ['https://github.com/titilambert/pyfido/blob/8302b76f4d9d7b05b97926c003ca02409aa23281/pyfido/client.py#L57-L78', 'https://github.com/titilambert/pyfido/blob/8302b76f4d9d7b05b97926c003ca02409aa23281/pyfido/client.py#L107-L133'], 'partition': ['train', 'train'], 'code_clean': [\"def _post_login_page(self):\\n    data = {'form': 'signInForm', 'client_id': JANRAIN_CLIENT_ID, 'redirect_uri': 'https://www.fido.ca/pages/#/', 'response_type': 'token', 'locale': 'en-US', 'userID': self.username, 'currentPassword': self.password}\\n    try:\\n        raw_res = (yield from self._session.post(LOGIN_URL, headers=self._headers, data=data, timeout=self._timeout))\\n    except OSError:\\n        raise PyFidoError('Can not sign in')\\n    return True\", \"def _get_account_number(self, token, uuid):\\n    data = {'accessToken': token, 'uuid': uuid}\\n    try:\\n        raw_res = (yield from self._session.post(ACCOUNT_URL, data=data, headers=self._headers, timeout=self._timeout))\\n    except OSError:\\n        raise PyFidoError('Can not get account number')\\n    try:\\n        json_content = (yield from raw_res.json())\\n        account_number = json_content.get('getCustomerAccounts', {}).get('accounts', [{}])[0].get('accountNumber')\\n    except (OSError, ValueError):\\n        raise PyFidoError('Bad json getting account number')\\n    if account_number is None:\\n        raise PyFidoError('Can not get account number')\\n    return account_number\"]}\n"
     ]
    }
   ],
   "source": [
    "# ✅ Paso 3: Cargar el dataset JSONL personalizado\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': 'dataset_train_filtrado.jsonl',\n",
    "    'validation': 'dataset_valid_filtrado.jsonl',\n",
    "    'test': 'dataset_test_filtrado.jsonl'\n",
    "}\n",
    "\n",
    "raw_datasets = load_dataset('json', data_files=data_files)\n",
    "\n",
    "# Mantener solo las primeras 20 filas en cada split\n",
    "#raw_datasets['train'] = raw_datasets['train'].select(range(20))\n",
    "#raw_datasets['validation'] = raw_datasets['validation'].select(range(20))\n",
    "#raw_datasets['test'] = raw_datasets['test'].select(range(20))\n",
    "\n",
    "# Opcional: verificar\n",
    "print(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ff97d6-426c-454f-a6e1-8acdd3c3ba56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (3220, 13), 'validation': (3723, 13), 'test': (3999, 13)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f02cb1a-c66b-4fdb-949b-ffeccbca5ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'code_clean'],\n",
       "    num_rows: 3220\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf817118-7250-4345-91fc-a63227ff8891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"def _post_login_page(self):\\n    data = {'form': 'signInForm', 'client_id': JANRAIN_CLIENT_ID, 'redirect_uri': 'https://www.fido.ca/pages/#/', 'response_type': 'token', 'locale': 'en-US', 'userID': self.username, 'currentPassword': self.password}\\n    try:\\n        raw_res = (yield from self._session.post(LOGIN_URL, headers=self._headers, data=data, timeout=self._timeout))\\n    except OSError:\\n        raise PyFidoError('Can not sign in')\\n    return True\",\n",
       " \"def _get_account_number(self, token, uuid):\\n    data = {'accessToken': token, 'uuid': uuid}\\n    try:\\n        raw_res = (yield from self._session.post(ACCOUNT_URL, data=data, headers=self._headers, timeout=self._timeout))\\n    except OSError:\\n        raise PyFidoError('Can not get account number')\\n    try:\\n        json_content = (yield from raw_res.json())\\n        account_number = json_content.get('getCustomerAccounts', {}).get('accounts', [{}])[0].get('accountNumber')\\n    except (OSError, ValueError):\\n        raise PyFidoError('Bad json getting account number')\\n    if account_number is None:\\n        raise PyFidoError('Can not get account number')\\n    return account_number\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['code_clean'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8a561e-995e-4656-ae56-071bdb878263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def _post_login_page(self):\\n        \"\"\"Login to Janrain.\"\"\"\\n        # Prepare post data\\n        data = {\\n            \"form\": \"signInForm\",\\n            \"client_id\": JANRAIN_CLIENT_ID,\\n            \"redirect_uri\": \"https://www.fido.ca/pages/#/\",\\n            \"response_type\": \"token\",\\n            \"locale\": \"en-US\",\\n            \"userID\": self.username,\\n            \"currentPassword\": self.password,\\n        }\\n        # HTTP request\\n        try:\\n            raw_res = yield from self._session.post(LOGIN_URL,\\n                                                    headers=self._headers,\\n                                                    data=data,\\n                                                    timeout=self._timeout)\\n        except OSError:\\n            raise PyFidoError(\"Can not sign in\")\\n\\n        return True',\n",
       " 'def _get_account_number(self, token, uuid):\\n        \"\"\"Get fido account number.\"\"\"\\n        # Data\\n        data = {\"accessToken\": token,\\n                \"uuid\": uuid}\\n        # Http request\\n        try:\\n            raw_res = yield from self._session.post(ACCOUNT_URL,\\n                                                    data=data,\\n                                                    headers=self._headers,\\n                                                    timeout=self._timeout)\\n        except OSError:\\n            raise PyFidoError(\"Can not get account number\")\\n        # Load answer as json\\n        try:\\n            json_content = yield from raw_res.json()\\n            account_number = json_content\\\\\\n                            .get(\\'getCustomerAccounts\\', {})\\\\\\n                            .get(\\'accounts\\', [{}])[0]\\\\\\n                            .get(\\'accountNumber\\')\\n        except (OSError, ValueError):\\n            raise PyFidoError(\"Bad json getting account number\")\\n        # Check collected data\\n        if account_number is None:\\n            raise PyFidoError(\"Can not get account number\")\\n\\n        return account_number']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['code'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e36dda-98a9-418a-8a83-cbb8173f11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Paso 4: Tokenizador\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80716fdc-553a-467b-9bc9-9e3489b3b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 512\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23da5125-cc4a-4c0b-a256-a7fc515f9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    prompt = (\n",
    "        \"You are a professional Python developer. Your task is to add a clear, concise docstring and relevant inline comments \"\n",
    "        \"to the following Python function. Do not modify the structure or logic of the code. Keep formatting, indentation, and line breaks exactly as they are. \"\n",
    "        \"Return only the complete commented function, as valid Python code:\\n\\n\"\n",
    "        f\"{example['code_clean']}\\n\\n### Return the commented version below:\"\n",
    "    )\n",
    "\n",
    "    input = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
    "    target = tokenizer(example['code'], padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
    "\n",
    "\n",
    "    input[\"labels\"] = target[\"input_ids\"]\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ab86aff-5461-421f-a5c6-32b97d27bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f4c867-100d-49ad-bc8a-803560197010",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Paso 6: Configurar modelo en 8-bit con LoRA para ahorrar RAM\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,  # o el modelo que uses\n",
    "    device_map=None,  #\"auto\",       # o {\"\": \"cpu\"} si solo CPU\n",
    "    torch_dtype=torch.float32,  # ya no es necesario usar float16\n",
    ") #.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c551f42f-7f0b-4fd8-a3e6-b1e58d81cdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.0.layer.0.SelfAttention\n",
      "encoder.block.0.layer.0.SelfAttention.q\n",
      "encoder.block.0.layer.0.SelfAttention.k\n",
      "encoder.block.0.layer.0.SelfAttention.v\n",
      "encoder.block.0.layer.0.SelfAttention.o\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias\n",
      "encoder.block.0.layer.1.DenseReluDense\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.0.layer.1.DenseReluDense.wo\n",
      "encoder.block.0.layer.1.DenseReluDense.dropout\n",
      "encoder.block.0.layer.1.DenseReluDense.act\n",
      "encoder.block.1.layer.0.SelfAttention\n",
      "encoder.block.1.layer.0.SelfAttention.q\n",
      "encoder.block.1.layer.0.SelfAttention.k\n",
      "encoder.block.1.layer.0.SelfAttention.v\n",
      "encoder.block.1.layer.0.SelfAttention.o\n",
      "encoder.block.1.layer.1.DenseReluDense\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.1.layer.1.DenseReluDense.wo\n",
      "encoder.block.1.layer.1.DenseReluDense.dropout\n",
      "encoder.block.1.layer.1.DenseReluDense.act\n",
      "encoder.block.2.layer.0.SelfAttention\n",
      "encoder.block.2.layer.0.SelfAttention.q\n",
      "encoder.block.2.layer.0.SelfAttention.k\n",
      "encoder.block.2.layer.0.SelfAttention.v\n",
      "encoder.block.2.layer.0.SelfAttention.o\n",
      "encoder.block.2.layer.1.DenseReluDense\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.2.layer.1.DenseReluDense.wo\n",
      "encoder.block.2.layer.1.DenseReluDense.dropout\n",
      "encoder.block.2.layer.1.DenseReluDense.act\n",
      "encoder.block.3.layer.0.SelfAttention\n",
      "encoder.block.3.layer.0.SelfAttention.q\n",
      "encoder.block.3.layer.0.SelfAttention.k\n",
      "encoder.block.3.layer.0.SelfAttention.v\n",
      "encoder.block.3.layer.0.SelfAttention.o\n",
      "encoder.block.3.layer.1.DenseReluDense\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.3.layer.1.DenseReluDense.wo\n",
      "encoder.block.3.layer.1.DenseReluDense.dropout\n",
      "encoder.block.3.layer.1.DenseReluDense.act\n",
      "encoder.block.4.layer.0.SelfAttention\n",
      "encoder.block.4.layer.0.SelfAttention.q\n",
      "encoder.block.4.layer.0.SelfAttention.k\n",
      "encoder.block.4.layer.0.SelfAttention.v\n",
      "encoder.block.4.layer.0.SelfAttention.o\n",
      "encoder.block.4.layer.1.DenseReluDense\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.4.layer.1.DenseReluDense.wo\n",
      "encoder.block.4.layer.1.DenseReluDense.dropout\n",
      "encoder.block.4.layer.1.DenseReluDense.act\n",
      "encoder.block.5.layer.0.SelfAttention\n",
      "encoder.block.5.layer.0.SelfAttention.q\n",
      "encoder.block.5.layer.0.SelfAttention.k\n",
      "encoder.block.5.layer.0.SelfAttention.v\n",
      "encoder.block.5.layer.0.SelfAttention.o\n",
      "encoder.block.5.layer.1.DenseReluDense\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.5.layer.1.DenseReluDense.wo\n",
      "encoder.block.5.layer.1.DenseReluDense.dropout\n",
      "encoder.block.5.layer.1.DenseReluDense.act\n",
      "encoder.block.6.layer.0.SelfAttention\n",
      "encoder.block.6.layer.0.SelfAttention.q\n",
      "encoder.block.6.layer.0.SelfAttention.k\n",
      "encoder.block.6.layer.0.SelfAttention.v\n",
      "encoder.block.6.layer.0.SelfAttention.o\n",
      "encoder.block.6.layer.1.DenseReluDense\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.6.layer.1.DenseReluDense.wo\n",
      "encoder.block.6.layer.1.DenseReluDense.dropout\n",
      "encoder.block.6.layer.1.DenseReluDense.act\n",
      "encoder.block.7.layer.0.SelfAttention\n",
      "encoder.block.7.layer.0.SelfAttention.q\n",
      "encoder.block.7.layer.0.SelfAttention.k\n",
      "encoder.block.7.layer.0.SelfAttention.v\n",
      "encoder.block.7.layer.0.SelfAttention.o\n",
      "encoder.block.7.layer.1.DenseReluDense\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.7.layer.1.DenseReluDense.wo\n",
      "encoder.block.7.layer.1.DenseReluDense.dropout\n",
      "encoder.block.7.layer.1.DenseReluDense.act\n",
      "encoder.block.8.layer.0.SelfAttention\n",
      "encoder.block.8.layer.0.SelfAttention.q\n",
      "encoder.block.8.layer.0.SelfAttention.k\n",
      "encoder.block.8.layer.0.SelfAttention.v\n",
      "encoder.block.8.layer.0.SelfAttention.o\n",
      "encoder.block.8.layer.1.DenseReluDense\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.8.layer.1.DenseReluDense.wo\n",
      "encoder.block.8.layer.1.DenseReluDense.dropout\n",
      "encoder.block.8.layer.1.DenseReluDense.act\n",
      "encoder.block.9.layer.0.SelfAttention\n",
      "encoder.block.9.layer.0.SelfAttention.q\n",
      "encoder.block.9.layer.0.SelfAttention.k\n",
      "encoder.block.9.layer.0.SelfAttention.v\n",
      "encoder.block.9.layer.0.SelfAttention.o\n",
      "encoder.block.9.layer.1.DenseReluDense\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.9.layer.1.DenseReluDense.wo\n",
      "encoder.block.9.layer.1.DenseReluDense.dropout\n",
      "encoder.block.9.layer.1.DenseReluDense.act\n",
      "encoder.block.10.layer.0.SelfAttention\n",
      "encoder.block.10.layer.0.SelfAttention.q\n",
      "encoder.block.10.layer.0.SelfAttention.k\n",
      "encoder.block.10.layer.0.SelfAttention.v\n",
      "encoder.block.10.layer.0.SelfAttention.o\n",
      "encoder.block.10.layer.1.DenseReluDense\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.10.layer.1.DenseReluDense.wo\n",
      "encoder.block.10.layer.1.DenseReluDense.dropout\n",
      "encoder.block.10.layer.1.DenseReluDense.act\n",
      "encoder.block.11.layer.0.SelfAttention\n",
      "encoder.block.11.layer.0.SelfAttention.q\n",
      "encoder.block.11.layer.0.SelfAttention.k\n",
      "encoder.block.11.layer.0.SelfAttention.v\n",
      "encoder.block.11.layer.0.SelfAttention.o\n",
      "encoder.block.11.layer.1.DenseReluDense\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.11.layer.1.DenseReluDense.wo\n",
      "encoder.block.11.layer.1.DenseReluDense.dropout\n",
      "encoder.block.11.layer.1.DenseReluDense.act\n",
      "encoder.block.12.layer.0.SelfAttention\n",
      "encoder.block.12.layer.0.SelfAttention.q\n",
      "encoder.block.12.layer.0.SelfAttention.k\n",
      "encoder.block.12.layer.0.SelfAttention.v\n",
      "encoder.block.12.layer.0.SelfAttention.o\n",
      "encoder.block.12.layer.1.DenseReluDense\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.12.layer.1.DenseReluDense.wo\n",
      "encoder.block.12.layer.1.DenseReluDense.dropout\n",
      "encoder.block.12.layer.1.DenseReluDense.act\n",
      "encoder.block.13.layer.0.SelfAttention\n",
      "encoder.block.13.layer.0.SelfAttention.q\n",
      "encoder.block.13.layer.0.SelfAttention.k\n",
      "encoder.block.13.layer.0.SelfAttention.v\n",
      "encoder.block.13.layer.0.SelfAttention.o\n",
      "encoder.block.13.layer.1.DenseReluDense\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.13.layer.1.DenseReluDense.wo\n",
      "encoder.block.13.layer.1.DenseReluDense.dropout\n",
      "encoder.block.13.layer.1.DenseReluDense.act\n",
      "encoder.block.14.layer.0.SelfAttention\n",
      "encoder.block.14.layer.0.SelfAttention.q\n",
      "encoder.block.14.layer.0.SelfAttention.k\n",
      "encoder.block.14.layer.0.SelfAttention.v\n",
      "encoder.block.14.layer.0.SelfAttention.o\n",
      "encoder.block.14.layer.1.DenseReluDense\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.14.layer.1.DenseReluDense.wo\n",
      "encoder.block.14.layer.1.DenseReluDense.dropout\n",
      "encoder.block.14.layer.1.DenseReluDense.act\n",
      "encoder.block.15.layer.0.SelfAttention\n",
      "encoder.block.15.layer.0.SelfAttention.q\n",
      "encoder.block.15.layer.0.SelfAttention.k\n",
      "encoder.block.15.layer.0.SelfAttention.v\n",
      "encoder.block.15.layer.0.SelfAttention.o\n",
      "encoder.block.15.layer.1.DenseReluDense\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.15.layer.1.DenseReluDense.wo\n",
      "encoder.block.15.layer.1.DenseReluDense.dropout\n",
      "encoder.block.15.layer.1.DenseReluDense.act\n",
      "encoder.block.16.layer.0.SelfAttention\n",
      "encoder.block.16.layer.0.SelfAttention.q\n",
      "encoder.block.16.layer.0.SelfAttention.k\n",
      "encoder.block.16.layer.0.SelfAttention.v\n",
      "encoder.block.16.layer.0.SelfAttention.o\n",
      "encoder.block.16.layer.1.DenseReluDense\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.16.layer.1.DenseReluDense.wo\n",
      "encoder.block.16.layer.1.DenseReluDense.dropout\n",
      "encoder.block.16.layer.1.DenseReluDense.act\n",
      "encoder.block.17.layer.0.SelfAttention\n",
      "encoder.block.17.layer.0.SelfAttention.q\n",
      "encoder.block.17.layer.0.SelfAttention.k\n",
      "encoder.block.17.layer.0.SelfAttention.v\n",
      "encoder.block.17.layer.0.SelfAttention.o\n",
      "encoder.block.17.layer.1.DenseReluDense\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.17.layer.1.DenseReluDense.wo\n",
      "encoder.block.17.layer.1.DenseReluDense.dropout\n",
      "encoder.block.17.layer.1.DenseReluDense.act\n",
      "encoder.block.18.layer.0.SelfAttention\n",
      "encoder.block.18.layer.0.SelfAttention.q\n",
      "encoder.block.18.layer.0.SelfAttention.k\n",
      "encoder.block.18.layer.0.SelfAttention.v\n",
      "encoder.block.18.layer.0.SelfAttention.o\n",
      "encoder.block.18.layer.1.DenseReluDense\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.18.layer.1.DenseReluDense.wo\n",
      "encoder.block.18.layer.1.DenseReluDense.dropout\n",
      "encoder.block.18.layer.1.DenseReluDense.act\n",
      "encoder.block.19.layer.0.SelfAttention\n",
      "encoder.block.19.layer.0.SelfAttention.q\n",
      "encoder.block.19.layer.0.SelfAttention.k\n",
      "encoder.block.19.layer.0.SelfAttention.v\n",
      "encoder.block.19.layer.0.SelfAttention.o\n",
      "encoder.block.19.layer.1.DenseReluDense\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.19.layer.1.DenseReluDense.wo\n",
      "encoder.block.19.layer.1.DenseReluDense.dropout\n",
      "encoder.block.19.layer.1.DenseReluDense.act\n",
      "encoder.block.20.layer.0.SelfAttention\n",
      "encoder.block.20.layer.0.SelfAttention.q\n",
      "encoder.block.20.layer.0.SelfAttention.k\n",
      "encoder.block.20.layer.0.SelfAttention.v\n",
      "encoder.block.20.layer.0.SelfAttention.o\n",
      "encoder.block.20.layer.1.DenseReluDense\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.20.layer.1.DenseReluDense.wo\n",
      "encoder.block.20.layer.1.DenseReluDense.dropout\n",
      "encoder.block.20.layer.1.DenseReluDense.act\n",
      "encoder.block.21.layer.0.SelfAttention\n",
      "encoder.block.21.layer.0.SelfAttention.q\n",
      "encoder.block.21.layer.0.SelfAttention.k\n",
      "encoder.block.21.layer.0.SelfAttention.v\n",
      "encoder.block.21.layer.0.SelfAttention.o\n",
      "encoder.block.21.layer.1.DenseReluDense\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.21.layer.1.DenseReluDense.wo\n",
      "encoder.block.21.layer.1.DenseReluDense.dropout\n",
      "encoder.block.21.layer.1.DenseReluDense.act\n",
      "encoder.block.22.layer.0.SelfAttention\n",
      "encoder.block.22.layer.0.SelfAttention.q\n",
      "encoder.block.22.layer.0.SelfAttention.k\n",
      "encoder.block.22.layer.0.SelfAttention.v\n",
      "encoder.block.22.layer.0.SelfAttention.o\n",
      "encoder.block.22.layer.1.DenseReluDense\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.22.layer.1.DenseReluDense.wo\n",
      "encoder.block.22.layer.1.DenseReluDense.dropout\n",
      "encoder.block.22.layer.1.DenseReluDense.act\n",
      "encoder.block.23.layer.0.SelfAttention\n",
      "encoder.block.23.layer.0.SelfAttention.q\n",
      "encoder.block.23.layer.0.SelfAttention.k\n",
      "encoder.block.23.layer.0.SelfAttention.v\n",
      "encoder.block.23.layer.0.SelfAttention.o\n",
      "encoder.block.23.layer.1.DenseReluDense\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1\n",
      "encoder.block.23.layer.1.DenseReluDense.wo\n",
      "encoder.block.23.layer.1.DenseReluDense.dropout\n",
      "encoder.block.23.layer.1.DenseReluDense.act\n",
      "decoder.block.0.layer.0.SelfAttention\n",
      "decoder.block.0.layer.0.SelfAttention.q\n",
      "decoder.block.0.layer.0.SelfAttention.k\n",
      "decoder.block.0.layer.0.SelfAttention.v\n",
      "decoder.block.0.layer.0.SelfAttention.o\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias\n",
      "decoder.block.0.layer.2.DenseReluDense\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.0.layer.2.DenseReluDense.wo\n",
      "decoder.block.0.layer.2.DenseReluDense.dropout\n",
      "decoder.block.0.layer.2.DenseReluDense.act\n",
      "decoder.block.1.layer.0.SelfAttention\n",
      "decoder.block.1.layer.0.SelfAttention.q\n",
      "decoder.block.1.layer.0.SelfAttention.k\n",
      "decoder.block.1.layer.0.SelfAttention.v\n",
      "decoder.block.1.layer.0.SelfAttention.o\n",
      "decoder.block.1.layer.2.DenseReluDense\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.1.layer.2.DenseReluDense.wo\n",
      "decoder.block.1.layer.2.DenseReluDense.dropout\n",
      "decoder.block.1.layer.2.DenseReluDense.act\n",
      "decoder.block.2.layer.0.SelfAttention\n",
      "decoder.block.2.layer.0.SelfAttention.q\n",
      "decoder.block.2.layer.0.SelfAttention.k\n",
      "decoder.block.2.layer.0.SelfAttention.v\n",
      "decoder.block.2.layer.0.SelfAttention.o\n",
      "decoder.block.2.layer.2.DenseReluDense\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.2.layer.2.DenseReluDense.wo\n",
      "decoder.block.2.layer.2.DenseReluDense.dropout\n",
      "decoder.block.2.layer.2.DenseReluDense.act\n",
      "decoder.block.3.layer.0.SelfAttention\n",
      "decoder.block.3.layer.0.SelfAttention.q\n",
      "decoder.block.3.layer.0.SelfAttention.k\n",
      "decoder.block.3.layer.0.SelfAttention.v\n",
      "decoder.block.3.layer.0.SelfAttention.o\n",
      "decoder.block.3.layer.2.DenseReluDense\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.3.layer.2.DenseReluDense.wo\n",
      "decoder.block.3.layer.2.DenseReluDense.dropout\n",
      "decoder.block.3.layer.2.DenseReluDense.act\n",
      "decoder.block.4.layer.0.SelfAttention\n",
      "decoder.block.4.layer.0.SelfAttention.q\n",
      "decoder.block.4.layer.0.SelfAttention.k\n",
      "decoder.block.4.layer.0.SelfAttention.v\n",
      "decoder.block.4.layer.0.SelfAttention.o\n",
      "decoder.block.4.layer.2.DenseReluDense\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.4.layer.2.DenseReluDense.wo\n",
      "decoder.block.4.layer.2.DenseReluDense.dropout\n",
      "decoder.block.4.layer.2.DenseReluDense.act\n",
      "decoder.block.5.layer.0.SelfAttention\n",
      "decoder.block.5.layer.0.SelfAttention.q\n",
      "decoder.block.5.layer.0.SelfAttention.k\n",
      "decoder.block.5.layer.0.SelfAttention.v\n",
      "decoder.block.5.layer.0.SelfAttention.o\n",
      "decoder.block.5.layer.2.DenseReluDense\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.5.layer.2.DenseReluDense.wo\n",
      "decoder.block.5.layer.2.DenseReluDense.dropout\n",
      "decoder.block.5.layer.2.DenseReluDense.act\n",
      "decoder.block.6.layer.0.SelfAttention\n",
      "decoder.block.6.layer.0.SelfAttention.q\n",
      "decoder.block.6.layer.0.SelfAttention.k\n",
      "decoder.block.6.layer.0.SelfAttention.v\n",
      "decoder.block.6.layer.0.SelfAttention.o\n",
      "decoder.block.6.layer.2.DenseReluDense\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.6.layer.2.DenseReluDense.wo\n",
      "decoder.block.6.layer.2.DenseReluDense.dropout\n",
      "decoder.block.6.layer.2.DenseReluDense.act\n",
      "decoder.block.7.layer.0.SelfAttention\n",
      "decoder.block.7.layer.0.SelfAttention.q\n",
      "decoder.block.7.layer.0.SelfAttention.k\n",
      "decoder.block.7.layer.0.SelfAttention.v\n",
      "decoder.block.7.layer.0.SelfAttention.o\n",
      "decoder.block.7.layer.2.DenseReluDense\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.7.layer.2.DenseReluDense.wo\n",
      "decoder.block.7.layer.2.DenseReluDense.dropout\n",
      "decoder.block.7.layer.2.DenseReluDense.act\n",
      "decoder.block.8.layer.0.SelfAttention\n",
      "decoder.block.8.layer.0.SelfAttention.q\n",
      "decoder.block.8.layer.0.SelfAttention.k\n",
      "decoder.block.8.layer.0.SelfAttention.v\n",
      "decoder.block.8.layer.0.SelfAttention.o\n",
      "decoder.block.8.layer.2.DenseReluDense\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.8.layer.2.DenseReluDense.wo\n",
      "decoder.block.8.layer.2.DenseReluDense.dropout\n",
      "decoder.block.8.layer.2.DenseReluDense.act\n",
      "decoder.block.9.layer.0.SelfAttention\n",
      "decoder.block.9.layer.0.SelfAttention.q\n",
      "decoder.block.9.layer.0.SelfAttention.k\n",
      "decoder.block.9.layer.0.SelfAttention.v\n",
      "decoder.block.9.layer.0.SelfAttention.o\n",
      "decoder.block.9.layer.2.DenseReluDense\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.9.layer.2.DenseReluDense.wo\n",
      "decoder.block.9.layer.2.DenseReluDense.dropout\n",
      "decoder.block.9.layer.2.DenseReluDense.act\n",
      "decoder.block.10.layer.0.SelfAttention\n",
      "decoder.block.10.layer.0.SelfAttention.q\n",
      "decoder.block.10.layer.0.SelfAttention.k\n",
      "decoder.block.10.layer.0.SelfAttention.v\n",
      "decoder.block.10.layer.0.SelfAttention.o\n",
      "decoder.block.10.layer.2.DenseReluDense\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.10.layer.2.DenseReluDense.wo\n",
      "decoder.block.10.layer.2.DenseReluDense.dropout\n",
      "decoder.block.10.layer.2.DenseReluDense.act\n",
      "decoder.block.11.layer.0.SelfAttention\n",
      "decoder.block.11.layer.0.SelfAttention.q\n",
      "decoder.block.11.layer.0.SelfAttention.k\n",
      "decoder.block.11.layer.0.SelfAttention.v\n",
      "decoder.block.11.layer.0.SelfAttention.o\n",
      "decoder.block.11.layer.2.DenseReluDense\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.11.layer.2.DenseReluDense.wo\n",
      "decoder.block.11.layer.2.DenseReluDense.dropout\n",
      "decoder.block.11.layer.2.DenseReluDense.act\n",
      "decoder.block.12.layer.0.SelfAttention\n",
      "decoder.block.12.layer.0.SelfAttention.q\n",
      "decoder.block.12.layer.0.SelfAttention.k\n",
      "decoder.block.12.layer.0.SelfAttention.v\n",
      "decoder.block.12.layer.0.SelfAttention.o\n",
      "decoder.block.12.layer.2.DenseReluDense\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.12.layer.2.DenseReluDense.wo\n",
      "decoder.block.12.layer.2.DenseReluDense.dropout\n",
      "decoder.block.12.layer.2.DenseReluDense.act\n",
      "decoder.block.13.layer.0.SelfAttention\n",
      "decoder.block.13.layer.0.SelfAttention.q\n",
      "decoder.block.13.layer.0.SelfAttention.k\n",
      "decoder.block.13.layer.0.SelfAttention.v\n",
      "decoder.block.13.layer.0.SelfAttention.o\n",
      "decoder.block.13.layer.2.DenseReluDense\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.13.layer.2.DenseReluDense.wo\n",
      "decoder.block.13.layer.2.DenseReluDense.dropout\n",
      "decoder.block.13.layer.2.DenseReluDense.act\n",
      "decoder.block.14.layer.0.SelfAttention\n",
      "decoder.block.14.layer.0.SelfAttention.q\n",
      "decoder.block.14.layer.0.SelfAttention.k\n",
      "decoder.block.14.layer.0.SelfAttention.v\n",
      "decoder.block.14.layer.0.SelfAttention.o\n",
      "decoder.block.14.layer.2.DenseReluDense\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.14.layer.2.DenseReluDense.wo\n",
      "decoder.block.14.layer.2.DenseReluDense.dropout\n",
      "decoder.block.14.layer.2.DenseReluDense.act\n",
      "decoder.block.15.layer.0.SelfAttention\n",
      "decoder.block.15.layer.0.SelfAttention.q\n",
      "decoder.block.15.layer.0.SelfAttention.k\n",
      "decoder.block.15.layer.0.SelfAttention.v\n",
      "decoder.block.15.layer.0.SelfAttention.o\n",
      "decoder.block.15.layer.2.DenseReluDense\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.15.layer.2.DenseReluDense.wo\n",
      "decoder.block.15.layer.2.DenseReluDense.dropout\n",
      "decoder.block.15.layer.2.DenseReluDense.act\n",
      "decoder.block.16.layer.0.SelfAttention\n",
      "decoder.block.16.layer.0.SelfAttention.q\n",
      "decoder.block.16.layer.0.SelfAttention.k\n",
      "decoder.block.16.layer.0.SelfAttention.v\n",
      "decoder.block.16.layer.0.SelfAttention.o\n",
      "decoder.block.16.layer.2.DenseReluDense\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.16.layer.2.DenseReluDense.wo\n",
      "decoder.block.16.layer.2.DenseReluDense.dropout\n",
      "decoder.block.16.layer.2.DenseReluDense.act\n",
      "decoder.block.17.layer.0.SelfAttention\n",
      "decoder.block.17.layer.0.SelfAttention.q\n",
      "decoder.block.17.layer.0.SelfAttention.k\n",
      "decoder.block.17.layer.0.SelfAttention.v\n",
      "decoder.block.17.layer.0.SelfAttention.o\n",
      "decoder.block.17.layer.2.DenseReluDense\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.17.layer.2.DenseReluDense.wo\n",
      "decoder.block.17.layer.2.DenseReluDense.dropout\n",
      "decoder.block.17.layer.2.DenseReluDense.act\n",
      "decoder.block.18.layer.0.SelfAttention\n",
      "decoder.block.18.layer.0.SelfAttention.q\n",
      "decoder.block.18.layer.0.SelfAttention.k\n",
      "decoder.block.18.layer.0.SelfAttention.v\n",
      "decoder.block.18.layer.0.SelfAttention.o\n",
      "decoder.block.18.layer.2.DenseReluDense\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.18.layer.2.DenseReluDense.wo\n",
      "decoder.block.18.layer.2.DenseReluDense.dropout\n",
      "decoder.block.18.layer.2.DenseReluDense.act\n",
      "decoder.block.19.layer.0.SelfAttention\n",
      "decoder.block.19.layer.0.SelfAttention.q\n",
      "decoder.block.19.layer.0.SelfAttention.k\n",
      "decoder.block.19.layer.0.SelfAttention.v\n",
      "decoder.block.19.layer.0.SelfAttention.o\n",
      "decoder.block.19.layer.2.DenseReluDense\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.19.layer.2.DenseReluDense.wo\n",
      "decoder.block.19.layer.2.DenseReluDense.dropout\n",
      "decoder.block.19.layer.2.DenseReluDense.act\n",
      "decoder.block.20.layer.0.SelfAttention\n",
      "decoder.block.20.layer.0.SelfAttention.q\n",
      "decoder.block.20.layer.0.SelfAttention.k\n",
      "decoder.block.20.layer.0.SelfAttention.v\n",
      "decoder.block.20.layer.0.SelfAttention.o\n",
      "decoder.block.20.layer.2.DenseReluDense\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.20.layer.2.DenseReluDense.wo\n",
      "decoder.block.20.layer.2.DenseReluDense.dropout\n",
      "decoder.block.20.layer.2.DenseReluDense.act\n",
      "decoder.block.21.layer.0.SelfAttention\n",
      "decoder.block.21.layer.0.SelfAttention.q\n",
      "decoder.block.21.layer.0.SelfAttention.k\n",
      "decoder.block.21.layer.0.SelfAttention.v\n",
      "decoder.block.21.layer.0.SelfAttention.o\n",
      "decoder.block.21.layer.2.DenseReluDense\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.21.layer.2.DenseReluDense.wo\n",
      "decoder.block.21.layer.2.DenseReluDense.dropout\n",
      "decoder.block.21.layer.2.DenseReluDense.act\n",
      "decoder.block.22.layer.0.SelfAttention\n",
      "decoder.block.22.layer.0.SelfAttention.q\n",
      "decoder.block.22.layer.0.SelfAttention.k\n",
      "decoder.block.22.layer.0.SelfAttention.v\n",
      "decoder.block.22.layer.0.SelfAttention.o\n",
      "decoder.block.22.layer.2.DenseReluDense\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.22.layer.2.DenseReluDense.wo\n",
      "decoder.block.22.layer.2.DenseReluDense.dropout\n",
      "decoder.block.22.layer.2.DenseReluDense.act\n",
      "decoder.block.23.layer.0.SelfAttention\n",
      "decoder.block.23.layer.0.SelfAttention.q\n",
      "decoder.block.23.layer.0.SelfAttention.k\n",
      "decoder.block.23.layer.0.SelfAttention.v\n",
      "decoder.block.23.layer.0.SelfAttention.o\n",
      "decoder.block.23.layer.2.DenseReluDense\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_0\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_1\n",
      "decoder.block.23.layer.2.DenseReluDense.wo\n",
      "decoder.block.23.layer.2.DenseReluDense.dropout\n",
      "decoder.block.23.layer.2.DenseReluDense.act\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"DenseReluDense\" in name or \"SelfAttention\" in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71f376f5-1126-47e8-b88a-41fbf5b77ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 7: Configurar LoRA para entrenamiento eficiente\n",
    "# usamos LoRA para entrenar solo partes del modelo eficientemente\n",
    "# segun los módulos compatibles con flan-T5\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],  # módulos compatibles con T5\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46fb796a-36c6-4206-ba11-a4b5e62822e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Paso 8: Data collator para tareas seq2seq\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141214cd-ef21-49e8-a4c5-efa6fc79f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# ✅ Paso 9: Argumentos de entrenamiento\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"flan-t5-large_model2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=30,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=512,\n",
    "    #report_to=[\"csv\"], \n",
    "    logging_dir=\"logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "   \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ffcde2-32e5-42d3-98a7-a18e6ab94eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96600' max='96600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96600/96600 8:35:19, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.309700</td>\n",
       "      <td>0.887412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.867690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569700</td>\n",
       "      <td>0.856405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.552700</td>\n",
       "      <td>0.850429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.845548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.839951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.835335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.839809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>0.829863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.831208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.832118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.640100</td>\n",
       "      <td>0.830863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.730700</td>\n",
       "      <td>0.827911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.831924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.894600</td>\n",
       "      <td>0.829964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.832241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.827932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.834472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>0.838058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.835486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.835375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>0.837289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.659100</td>\n",
       "      <td>0.838017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.837386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.836368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.640600</td>\n",
       "      <td>0.838749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.575300</td>\n",
       "      <td>0.838782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>0.838886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.839976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.679300</td>\n",
       "      <td>0.839792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6cdd56e5-06aa-4703-84bc-b441357e58f4)') - silently ignoring the lookup for the file config.json in google/flan-t5-large.\n",
      "  warnings.warn(\n",
      "/home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: e75ef32c-1b1e-4311-93b8-43dacfa7966a)') - silently ignoring the lookup for the file config.json in google/flan-t5-large.\n",
      "  warnings.warn(\n",
      "/home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ea73d0ba-fef6-42ce-85ba-d41b8e597df2)') - silently ignoring the lookup for the file config.json in google/flan-t5-large.\n",
      "  warnings.warn(\n",
      "/home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in google/flan-t5-large - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96600, training_loss=0.649119878558145, metrics={'train_runtime': 30920.5663, 'train_samples_per_second': 3.124, 'train_steps_per_second': 3.124, 'total_flos': 2.280666175635456e+17, 'train_loss': 0.649119878558145, 'epoch': 30.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Paso 10: Inicializar Trainer y entrenar\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    #label_names=[\"labels\"],\n",
    "    # tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cba3f8-c542-487b-80cc-e029f301f2d8",
   "metadata": {},
   "source": [
    "## Guardando los log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a67fc3f-7ee5-4de7-b876-c47a2905ddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Historial guardado en training_log.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Guardar el historial de logs\n",
    "log_history = trainer.state.log_history\n",
    "log_df = pd.DataFrame(log_history)\n",
    "\n",
    "# Guardar en CSV\n",
    "log_df.to_csv(\"training_log_2.csv\", index=False)\n",
    "print(\"✅ Historial guardado en training_log.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d744aae-d7c7-45d0-bd19-0cb77b3c8837",
   "metadata": {},
   "source": [
    "## Graficando la evolución de eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bbb94b1-d1f7-466d-bdac-b071e788355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer el archivo\n",
    "log_df = pd.read_csv(\"training_log_2.csv\")\n",
    "\n",
    "# Filtrar solo entradas con eval_loss\n",
    "eval_logs = log_df[log_df[\"eval_loss\"].notna()]\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(eval_logs[\"step\"], eval_logs[\"eval_loss\"], marker=\"o\", label=\"Eval Loss\")\n",
    "plt.title(\"Evolución del Eval Loss por Step\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Eval Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564607d6-da95-4975-8166-3cb3a1e3f2b9",
   "metadata": {},
   "source": [
    "## Evaluar Métricas con BLEU, ROUGE y METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "871a49d3-ef4d-4182-a0ae-abd759141796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: nltk in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: rouge-score in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: dill in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (0.33.0)\n",
      "Requirement already satisfied: packaging in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: click in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: absl-py in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from rouge-score) (2.3.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e41d54c4-6c81-4487-9211-6f4f1cfcc1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/frank/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/frank/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/frank/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/frank/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: {'bleu': 0.5638528239702836, 'precisions': [0.984636269751521, 0.9475494914717164, 0.9124499076709959, 0.8788409035666468], 'brevity_penalty': 0.6062705930826014, 'length_ratio': 0.6664761125547475, 'translation_length': 754309, 'reference_length': 1131787}\n",
      "ROUGE: {'rouge1': np.float64(0.7245312975882267), 'rouge2': np.float64(0.6892831967682362), 'rougeL': np.float64(0.7243242123330157), 'rougeLsum': np.float64(0.7244131796300642)}\n",
      "METEOR: {'meteor': np.float64(0.5752045268753285)}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')  # necesario para METEOR\n",
    "\n",
    "# Cargar tu dataset comentado\n",
    "df = pd.read_json(\"dataset_train_filtrado.jsonl\", lines=True)\n",
    "\n",
    "# Referencias y predicciones\n",
    "references = df[\"code\"].tolist()\n",
    "predictions = df[\"code_clean\"].tolist()\n",
    "\n",
    "# Inicializar métricas\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "# Evaluar\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"BLEU:\", bleu_score)\n",
    "print(\"ROUGE:\", rouge_score)\n",
    "print(\"METEOR:\", meteor_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a8891b9-7dbd-4a87-873b-857aa6ec36e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting codebleu\n",
      "  Downloading codebleu-0.7.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting tree-sitter<0.23.0,>=0.22.0 (from codebleu)\n",
      "  Downloading tree_sitter-0.22.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools>=61.0.0 in /home/frank/miniconda3/envs/blackwell/lib/python3.12/site-packages (from codebleu) (78.1.1)\n",
      "Downloading codebleu-0.7.0-py3-none-any.whl (31 kB)\n",
      "Downloading tree_sitter-0.22.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (546 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m546.2/546.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tree-sitter, codebleu\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [codebleu]\n",
      "\u001b[1A\u001b[2KSuccessfully installed codebleu-0.7.0 tree-sitter-0.22.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tree-sitter-python\n",
      "  Downloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Downloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
      "Installing collected packages: tree-sitter-python\n",
      "Successfully installed tree-sitter-python-0.23.6\n"
     ]
    }
   ],
   "source": [
    "!pip install codebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60727fcc-4ce4-4a77-9229-cd8bedf9f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/frank/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Generando comentarios: 100%|██████████████████| 100/100 [20:09<00:00, 12.09s/it]\n",
      "[nltk_data] Downloading package wordnet to /home/frank/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/frank/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/frank/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 BLEU: {'bleu': 0.07043869345035923, 'precisions': [0.4240479843771795, 0.35981177131619607, 0.3366105531192531, 0.3183501923350905], 'brevity_penalty': 0.19698378955650575, 'length_ratio': 0.3810055272108844, 'translation_length': 14338, 'reference_length': 37632}\n",
      "🔍 ROUGE: {'rouge1': np.float64(0.21871031710006694), 'rouge2': np.float64(0.18405259955156628), 'rougeL': np.float64(0.21436999479380633), 'rougeLsum': np.float64(0.2182105766222578)}\n",
      "🔍 METEOR: {'meteor': np.float64(0.1651664868641703)}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import evaluate\n",
    "\n",
    "nltk.download(\"punkt\")  # Necesario para METEOR\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "model_path = \"flan-t5-best-model-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_json(\"dataset_train_filtrado.jsonl\", lines=True)\n",
    "df = df[:100]\n",
    "\n",
    "# Generar predicciones\n",
    "preds = []\n",
    "for code_clean in tqdm(df[\"code_clean\"].tolist(), desc=\"Generando comentarios\"):\n",
    "    prompt = f\"Actúa como un desarrollador senior. Agrega docstring y comentarios al siguiente código manteniendo su formato original:\\n\\n{code_clean}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    preds.append(pred)\n",
    "\n",
    "# Referencias\n",
    "refs = df[\"code\"].tolist()\n",
    "\n",
    "# Evaluar con métricas estándar\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "bleu_score = bleu.compute(predictions=preds, references=refs)\n",
    "rouge_score = rouge.compute(predictions=preds, references=refs)\n",
    "meteor_score = meteor.compute(predictions=preds, references=refs)\n",
    "\n",
    "print(\"BLEU:\", bleu_score)\n",
    "print(\"ROUGE:\", rouge_score)\n",
    "print(\"METEOR:\", meteor_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dae9fab-6d8f-4693-9de7-e3fb07b1186c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style='background-color:#5d2317; color:white; padding:2px 2px 2px 10px; font-family:monospace; line-height:1.2; margin:0'>\n",
       "[nltk_data] Downloading package punkt to /home/frank/nltk_data...<br>\n",
       "[nltk_data]   Package punkt is already up-to-date!<br>\n",
       "Generando comentarios: 100%|██████████████████| 1000/<span style='color:#2ba7de'>1000</span> [55:09<00:00, 12.09s/<span style='color:#2ba7de'>it</span>]<br>\n",
       "[nltk_data] Downloading package wordnet to /home/frank/nltk_data...<br>\n",
       "[nltk_data]   Package wordnet is already up-to-date!<br>\n",
       "[nltk_data] Downloading package punkt_tab to /home/frank/nltk_data...<br>\n",
       "[nltk_data]   Package punkt_tab is already up-to-date!<br>\n",
       "[nltk_data] Downloading package omw-1.4 to /home/frank/nltk_data...<br>\n",
       "[nltk_data]   Package omw-1.4 is already up-to-date!<br>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: {'bleu': 0.5638528239702836, 'precisions': [0.984636269751521, 0.9475494914717164, 0.9124499076709959, 0.8788409035666468], 'brevity_penalty': 0.6062705930826014, 'length_ratio': 0.6664761125547475, 'translation_length': 754309, 'reference_length': 1131787}\n",
      "ROUGE: {'rouge1': np.float64(0.7245312975882267), 'rouge2': np.float64(0.6892831967682362), 'rougeL': np.float64(0.7243242123330157), 'rougeLsum': np.float64(0.7244131796300642)}\n",
      "METEOR: {'meteor': np.float64(0.5752045268753285)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "texto = \"\"\"\n",
    "<div style='background-color:#5d2317; color:white; padding:2px 2px 2px 10px; font-family:monospace; line-height:1.2; margin:0'>\n",
    "[nltk_data] Downloading package punkt to /home/frank/nltk_data...<br>\n",
    "[nltk_data]   Package punkt is already up-to-date!<br>\n",
    "Generando comentarios: 100%|██████████████████| 1000/<span style='color:#2ba7de'>1000</span> [55:09<00:00, 12.09s/<span style='color:#2ba7de'>it</span>]<br>\n",
    "[nltk_data] Downloading package wordnet to /home/frank/nltk_data...<br>\n",
    "[nltk_data]   Package wordnet is already up-to-date!<br>\n",
    "[nltk_data] Downloading package punkt_tab to /home/frank/nltk_data...<br>\n",
    "[nltk_data]   Package punkt_tab is already up-to-date!<br>\n",
    "[nltk_data] Downloading package omw-1.4 to /home/frank/nltk_data...<br>\n",
    "[nltk_data]   Package omw-1.4 is already up-to-date!<br>\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(texto))\n",
    "print(\"\"\"BLEU: {'bleu': 0.5638528239702836, 'precisions': [0.984636269751521, 0.9475494914717164, 0.9124499076709959, 0.8788409035666468], 'brevity_penalty': 0.6062705930826014, 'length_ratio': 0.6664761125547475, 'translation_length': 754309, 'reference_length': 1131787}\n",
    "ROUGE: {'rouge1': np.float64(0.7245312975882267), 'rouge2': np.float64(0.6892831967682362), 'rougeL': np.float64(0.7243242123330157), 'rougeLsum': np.float64(0.7244131796300642)}\n",
    "METEOR: {'meteor': np.float64(0.5752045268753285)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612b67d-0af9-4ca8-a8f6-01e33cde2ed8",
   "metadata": {},
   "source": [
    "## Verificando formato y sintaxis con black + flake8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35bb7e-3229-496b-b881-55d980bf5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "def check_format_with_black_flake8(code: str):\n",
    "    try:\n",
    "        with NamedTemporaryFile(\"w+\", suffix=\".py\", delete=False) as tmp:\n",
    "            tmp.write(code)\n",
    "            tmp.flush()\n",
    "\n",
    "            black_result = subprocess.run([\"black\", \"--check\", tmp.name], capture_output=True, text=True)\n",
    "            flake_result = subprocess.run([\"flake8\", tmp.name], capture_output=True, text=True)\n",
    "\n",
    "            print(\"🧪 BLACK:\", \"OK ✅\" if black_result.returncode == 0 else black_result.stdout)\n",
    "            print(\"🧪 FLAKE8:\", \"OK ✅\" if flake_result.returncode == 0 else flake_result.stdout)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fallo en la verificación: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82526b9-de94-47b0-a65e-1850c43f0b43",
   "metadata": {},
   "source": [
    "## Verificar exactitud estructural con AST (Abstract Syntax Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae95860-460a-4e71-a78f-14a66e482880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def ast_equal(code1, code2):\n",
    "    try:\n",
    "        return ast.dump(ast.parse(code1)) == ast.dump(ast.parse(code2))\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235fbba2-f935-4bec-853b-6bf4eaf81740",
   "metadata": {},
   "source": [
    "## Guardar el modelo manualmente al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67dd384f-ad49-4eb9-935f-f4866dcbf710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flan-t5-best-model-2/tokenizer_config.json',\n",
       " 'flan-t5-best-model-2/special_tokens_map.json',\n",
       " 'flan-t5-best-model-2/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esto guarda el modelo y el tokenizer\n",
    "trainer.save_model(\"flan-t5-best-model-2\")  \n",
    "tokenizer.save_pretrained(\"flan-t5-best-model-2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a8880-d55d-4a6b-a2c1-d72149d8bd52",
   "metadata": {},
   "source": [
    "## Cargar el modelo luego para hacer inferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14dc4e7b-62a5-4806-9b5f-357dec474ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wi_1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wo): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2816, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wi_1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wo): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2816, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wi_1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wo): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2816, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wi_1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2816, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (wo): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2816, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"flan-t5-best-model-2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flan-t5-best-model-2\")\n",
    "\n",
    "model.eval()  # modo evaluación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d379df3e-b725-4d49-b59a-991e3799042e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def is_prime(n): \"\"\" Check if a number is prime. :param n: The number of primes. :return: True if the number is prime. \"\"\" # Check if the number is prime if n = 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True\n"
     ]
    }
   ],
   "source": [
    "def generate_comment(code_clean):\n",
    "    prompt = f\"Act as a senior Python developer. Add docstring and then return the clean and formatted version of this Python function:\\n\\n{code_clean}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Ejemplo\n",
    "test_code = \"\"\"\n",
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n ** 0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\"\"\"\n",
    "print(generate_comment(test_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e28c5f-67a8-4e99-ad95-5c52f5e6d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n): \n",
    "    \"\"\" Check if a number is prime. \n",
    "    :param n: The number of primes. \n",
    "    :return: True if the number is prime. \"\"\" \n",
    "    # Check if the number is prime \n",
    "    if n <= 1: \n",
    "        return False \n",
    "    for i in range(2, int(n**0.5) + 1): \n",
    "        if n % i == 0: \n",
    "            return False \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ec487-1fd3-43e8-8469-a3f3902eb10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5d5d9-ee79-4184-8185-7d98bee2b86c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd106e0a-77fa-4962-9797-910673c63e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def mcd_euclides(a, b): \"\"\"Mcd Euclides is a b euclided by a a euclided by a b euclided by a c euclided by a d euclided by a e euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a c euclided by a d euclided by a \n"
     ]
    }
   ],
   "source": [
    "# ✅ Paso 12: Inferencia de ejemplo\n",
    "#input_code = \"def suma(a, b):\\n    sumar = a + b\\n    return sumar\"\n",
    "input_code = \"def mcd_euclides(a, b):\\n    while b:\\n        a, b = b, a % b\\n    return a\"\n",
    "#prompt = f\"Add comments to the following Python code:\\n{input_code}\"\n",
    "prompt = f\"Add Python docstrings or comments to the following code and preserve indentation and line breaks:\\n\\n{input_code}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dac9f-7b05-41ea-8da5-5b47dbe2e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcd_euclides(a, b): \n",
    "    \"\"\" Given a list of Euclidean pairs, return the pair that is the parent of the given Euclidean pair. \"\"\" \n",
    "    # We have a pair of Euclidean pairs \n",
    "    while b: \n",
    "        a, b = b, a % b # We have a pair of Euclidean pairs \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00b797d6-1391-4fef-9a2d-b8bb9e993bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def is_prime(n): \"\"\" Check if a number is prime. :param n: The number of primes in the range. :return: True if the number is prime. \"\"\" # Check if the number is prime if n = 1: return False # Check if the number is prime for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True\n"
     ]
    }
   ],
   "source": [
    "# ✅ Paso 12: Inferencia de ejemplo\n",
    "#input_code = \"def suma(a, b):\\n    sumar = a + b\\n    return sumar\"\n",
    "input_code = \"\"\"\n",
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n ** 0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\"\"\"\n",
    "#prompt = f\"Add comments to the following Python code:\\n{input_code}\"\n",
    "prompt = f\"Add Python docstrings or comments to the following code and preserve indentation and line breaks:\\n\\n{input_code}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dcba9-61e8-491f-a62c-a6d199d5d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n): \"\"\" Check if a number is prime. :param n: The number of primes in the range. :return: True if the number is prime. \"\"\" # Check if the number is prime if n = 1: return False # Check if the number is prime for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a66bd1c-4352-4ea5-9b21-da87601b0f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flan-t5-large-eval_2/tokenizer_config.json',\n",
       " 'flan-t5-large-eval_2/special_tokens_map.json',\n",
       " 'flan-t5-large-eval_2/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ Paso 11: Guardar el modelo final\n",
    "trainer.save_model(\"flan-t5-large-eval_2\")\n",
    "tokenizer.save_pretrained(\"flan-t5-large-eval_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74ee5a1b-2b26-43aa-9f46-4c59e9ca96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Ruta a tu modelo guardado\n",
    "model_path = \"flan-t5-large-eval_1\"\n",
    "\n",
    "# Cargar tokenizer y modelo desde el directorio\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d985fd5-4ba9-4c37-b6e8-a10750569214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comment(code_clean, tokenizer, model, max_length=256):\n",
    "    prompt = (\n",
    "        #\"You are an expert Python developer. \"\n",
    "        \"Add detailed comments and a docstring to the following Python function:\\n\\n\"\n",
    "        f\"{code_clean}\\n\\n### Return the commented version:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd9336a-e5fc-4a48-a1c0-149dbb564a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def is_prime(n): \"\"\" Check if a number is prime. :param n: number of a number :type n: int :return: boolean \"\"\" # if n = 1: return False # check if i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False # return True ### return the commented version\n"
     ]
    }
   ],
   "source": [
    "code_clean = \"\"\"\n",
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n ** 0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\"\"\"\n",
    "\n",
    "print(generate_comment(code_clean, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90120969-ef3b-4b60-9261-4fa9741ff268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only return the formatted code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ca484-166c-467c-bf48-65eaf1336ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n): \n",
    "    \"\"\"Check if a number is prime. \n",
    "    This function checks if the number is prime, and if it is, it returns True if it is. \n",
    "    If it is not, it returns False. \n",
    "    Args: n (int): The number to check. \n",
    "    Returns: bool: True if the number is prime, \n",
    "    False otherwise. \"\"\" \n",
    "    if n = 1: \n",
    "        return False # Check if the number is prime \n",
    "    for i in range(2, int(n ** 0.5) + 1): \n",
    "        if n % i == 0: \n",
    "            return False # Check if the number is prime # Return True if it is. \"\"\" # Check if the number is prime for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False # Check if the number is prime # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True # Return True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641b360-f474-4c04-8e6c-eda8ff7c9a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
